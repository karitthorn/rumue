{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Language Recognition using Deep Learning\n",
    "\n",
    "โปรเจกต์นี้สร้าง AI Model สำหรับจำแนก Sign Language โดยใช้ Sign Language MNIST Dataset\n",
    "\n",
    "## เนื้อหา\n",
    "1. ติดตั้ง Libraries และโหลด Dataset\n",
    "2. สำรวจข้อมูล (Exploratory Data Analysis)\n",
    "3. เตรียมข้อมูล (Data Preprocessing)\n",
    "4. สร้าง Model แบบ CNN\n",
    "5. สร้าง Model แบบ Transfer Learning\n",
    "6. Train และประเมินผล Models\n",
    "7. บันทึก Model และทดสอบ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries และ Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Alternative (สำหรับ Google Colab)**:  \nหากรันบน Colab และต้องการติดตั้งด้วยคำสั่งง่ายๆ ให้ใช้:\n```python\n!pip install -q kagglehub tensorflow numpy pandas matplotlib seaborn scikit-learn opencv-python Pillow\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ติดตั้ง dependencies ตาม requirements.txt\n# สำหรับ Google Colab หรือสภาพแวดล้อมใหม่\n\nimport sys\nimport subprocess\n\ndef install_requirements():\n    \"\"\"ติดตั้ง packages จากไฟล์ requirements.txt\"\"\"\n    try:\n        print(\"กำลังติดตั้ง dependencies...\")\n        print(\"=\"*60)\n        \n        # ติดตั้งจากไฟล์ requirements.txt\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\", \"-q\"])\n        \n        print(\"=\"*60)\n        print(\"✅ ติดตั้ง dependencies สำเร็จ!\")\n        print(\"\\nหมายเหตุ: ถ้าติดตั้งไว้แล้ว จะข้ามการติดตั้งซ้ำโดยอัตโนมัติ\")\n        \n    except FileNotFoundError:\n        print(\"⚠️  ไม่พบไฟล์ requirements.txt\")\n        print(\"กำลังติดตั้ง packages หลักๆ...\")\n        \n        # ติดตั้ง packages หลักๆ ถ้าไม่มีไฟล์ requirements.txt\n        packages = [\n            \"kagglehub\",\n            \"tensorflow>=2.13.0\",\n            \"numpy>=1.24.0\",\n            \"pandas>=2.0.0\",\n            \"matplotlib>=3.7.0\",\n            \"seaborn>=0.12.0\",\n            \"scikit-learn>=1.3.0\",\n            \"opencv-python>=4.8.0\",\n            \"Pillow>=10.0.0\"\n        ]\n        \n        for package in packages:\n            try:\n                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n                print(f\"✅ ติดตั้ง {package}\")\n            except:\n                print(f\"❌ ติดตั้ง {package} ล้มเหลว\")\n        \n        print(\"\\n✅ ติดตั้ง dependencies เสร็จสิ้น!\")\n    \n    except Exception as e:\n        print(f\"❌ เกิดข้อผิดพลาด: {str(e)}\")\n        print(\"\\nคุณสามารถติดตั้งด้วยตนเองโดยรันคำสั่ง:\")\n        print(\"!pip install -r requirements.txt\")\n\n# รันการติดตั้ง\ninstall_requirements()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 0. ติดตั้ง Dependencies (ถ้ายังไม่ได้ติดตั้ง)\n\n**หมายเหตุ**: ถ้าคุณรัน Notebook ใน Google Colab หรือสภาพแวดล้อมใหม่ ให้รัน cell นี้ก่อน  \nถ้าติดตั้ง dependencies ไว้แล้ว สามารถข้ามไปยัง Section 1 ได้เลย",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# ML Utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Kaggle\n",
    "import kagglehub\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Kaggle\n",
    "print(\"Downloading Sign Language MNIST dataset...\")\n",
    "path = kagglehub.dataset_download(\"datamunge/sign-language-mnist\")\n",
    "print(f\"Path to dataset files: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# โหลดข้อมูล\n",
    "train_df = pd.read_csv(os.path.join(path, 'sign_mnist_train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(path, 'sign_mnist_test.csv'))\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Testing data shape: {test_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดงข้อมูลสถิติ\n",
    "print(\"Dataset Information:\")\n",
    "print(f\"Number of training samples: {len(train_df)}\")\n",
    "print(f\"Number of test samples: {len(test_df)}\")\n",
    "print(f\"Number of features: {train_df.shape[1] - 1}\")  # ลบ label column\n",
    "print(f\"Number of classes: {train_df['label'].nunique()}\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง label mapping (A-Z ยกเว้น J และ Z)\n",
    "# Dataset มี 24 คลาส (0-24) ที่แทน A-Y ยกเว้น J\n",
    "label_map = {i: chr(65 + i) if i < 9 else chr(65 + i + 1) for i in range(24)}\n",
    "# 0=A, 1=B, ..., 8=I, 9=K, ..., 24=Y\n",
    "print(\"Label Mapping:\")\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดง class distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "sns.countplot(data=train_df, x='label')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.xlabel('Sign Language Letter')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(range(24), [label_map[i] for i in range(24)])\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดงตัวอย่างภาพจากแต่ละคลาส\n",
    "def display_sample_images(df, n_samples=24):\n",
    "    fig, axes = plt.subplots(4, 6, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # เลือกภาพแรกของแต่ละคลาส\n",
    "        sample = df[df['label'] == i].iloc[0]\n",
    "        pixels = sample[1:].values.reshape(28, 28)\n",
    "        \n",
    "        axes[i].imshow(pixels, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label_map[i]} ({i})')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Sample Images from Each Class', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "display_sample_images(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดงตัวอย่างหลายภาพจากคลาสเดียวกัน\n",
    "def display_multiple_samples(df, label_num, n_samples=9):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    samples = df[df['label'] == label_num].sample(n_samples)\n",
    "    \n",
    "    for i, (idx, sample) in enumerate(samples.iterrows()):\n",
    "        pixels = sample[1:].values.reshape(28, 28)\n",
    "        axes[i].imshow(pixels, cmap='gray')\n",
    "        axes[i].set_title(f'{label_map[label_num]} - Sample {i+1}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Multiple Samples of Letter \"{label_map[label_num]}\"', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# แสดงตัวอย่างหลายภาพของตัวอักษร 'A' (label 0)\n",
    "display_multiple_samples(train_df, label_num=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แยก features และ labels\n",
    "X_train = train_df.drop('label', axis=1).values\n",
    "y_train = train_df['label'].values\n",
    "\n",
    "X_test = test_df.drop('label', axis=1).values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "# Reshape เป็น 28x28x1 (grayscale)\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_test = X_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Normalize pixel values (0-255 -> 0-1)\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แบ่ง validation set จาก training set (80-20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y_train  # รักษาสัดส่วนของแต่ละคลาส\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "num_classes = 24\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val_cat = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(f\"One-hot encoded labels shape: {y_train_cat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation สำหรับ training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10,          # หมุนภาพ ±10 องศา\n",
    "    width_shift_range=0.1,      # เลื่อนภาพแนวนอน\n",
    "    height_shift_range=0.1,     # เลื่อนภาพแนวตั้ง\n",
    "    zoom_range=0.1,             # ซูมเข้า-ออก\n",
    "    shear_range=0.1,            # บิดภาพ\n",
    "    fill_mode='nearest'         # เติมพิกเซลที่ว่าง\n",
    ")\n",
    "\n",
    "# Validation/Test ไม่ต้อง augment\n",
    "val_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "print(\"Data augmentation configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดงตัวอย่าง augmented images\n",
    "def show_augmented_images(image, n_samples=9):\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image.reshape(28, 28), cmap='gray')\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Augmented images\n",
    "    image_reshaped = image.reshape(1, 28, 28, 1)\n",
    "    aug_iter = train_datagen.flow(image_reshaped, batch_size=1)\n",
    "    \n",
    "    for i in range(1, n_samples):\n",
    "        aug_image = next(aug_iter)[0]\n",
    "        axes[i].imshow(aug_image.reshape(28, 28), cmap='gray')\n",
    "        axes[i].set_title(f'Augmented {i}')\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Data Augmentation Examples', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# แสดงตัวอย่าง augmentation\n",
    "sample_image = X_train[0]\n",
    "show_augmented_images(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. สร้าง CNN Model (แบบพื้นฐาน)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape=(28, 28, 1), num_classes=24):\n",
    "    \"\"\"\n",
    "    สร้าง CNN Model พื้นฐานสำหรับจำแนก Sign Language\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # Conv Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape, padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output Layer\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# สร้าง model\n",
    "cnn_model = create_cnn_model()\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "cnn_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"CNN Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_cnn_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CNN Model\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "print(\"Starting training CNN model...\")\n",
    "history_cnn = cnn_model.fit(\n",
    "    train_datagen.flow(X_train, y_train_cat, batch_size=batch_size),\n",
    "    validation_data=(X_val, y_val_cat),\n",
    "    epochs=epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "def plot_training_history(history, title_prefix=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "    ax1.set_title(f'{title_prefix} Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss\n",
    "    ax2.plot(history.history['loss'], label='Train Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Val Loss')\n",
    "    ax2.set_title(f'{title_prefix} Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history_cnn, 'CNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. สร้าง Transfer Learning Model (MobileNetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เตรียมข้อมูลสำหรับ Transfer Learning (ต้องการ 3 channels)\n",
    "X_train_rgb = np.repeat(X_train, 3, axis=-1)\n",
    "X_val_rgb = np.repeat(X_val, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test, 3, axis=-1)\n",
    "\n",
    "# Resize เป็น 96x96 (ขนาดที่ MobileNetV2 รองรับ)\n",
    "X_train_resized = tf.image.resize(X_train_rgb, [96, 96]).numpy()\n",
    "X_val_resized = tf.image.resize(X_val_rgb, [96, 96]).numpy()\n",
    "X_test_resized = tf.image.resize(X_test_rgb, [96, 96]).numpy()\n",
    "\n",
    "print(f\"Resized training data shape: {X_train_resized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model(input_shape=(96, 96, 3), num_classes=24):\n",
    "    \"\"\"\n",
    "    สร้าง Transfer Learning Model โดยใช้ MobileNetV2\n",
    "    \"\"\"\n",
    "    # โหลด pre-trained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        input_shape=input_shape,\n",
    "        include_top=False,\n",
    "        weights='imagenet'\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # สร้าง model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# สร้าง model\n",
    "transfer_model, base_model = create_transfer_learning_model()\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile Transfer Learning model\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Transfer Learning Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks สำหรับ Transfer Learning\n",
    "tl_callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'best_transfer_model.keras',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation สำหรับ Transfer Learning\n",
    "tl_train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    zoom_range=0.15,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,  # ไม่ flip เพราะ Sign Language มีทิศทาง\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "print(\"Data augmentation for Transfer Learning configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Transfer Learning Model (Phase 1: Frozen base)\n",
    "print(\"Phase 1: Training with frozen base model...\")\n",
    "history_tl = transfer_model.fit(\n",
    "    tl_train_datagen.flow(X_train_resized, y_train_cat, batch_size=64),\n",
    "    validation_data=(X_val_resized, y_val_cat),\n",
    "    epochs=20,\n",
    "    callbacks=tl_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nPhase 1 training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning: Unfreeze ชั้นบนสุดของ base model\n",
    "print(\"Phase 2: Fine-tuning...\")\n",
    "base_model.trainable = True\n",
    "\n",
    "# Freeze ชั้นแรกๆ ไว้ (fine-tune เฉพาะชั้นบนสุด)\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile ใหม่ด้วย learning rate ต่ำ\n",
    "transfer_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(f\"Trainable layers: {sum([1 for layer in transfer_model.layers if layer.trainable])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Phase 2: Fine-tuning\n",
    "history_tl_ft = transfer_model.fit(\n",
    "    tl_train_datagen.flow(X_train_resized, y_train_cat, batch_size=64),\n",
    "    validation_data=(X_val_resized, y_val_cat),\n",
    "    epochs=20,\n",
    "    callbacks=tl_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nFine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Transfer Learning history\n",
    "plot_training_history(history_tl, 'Transfer Learning (Phase 1)')\n",
    "plot_training_history(history_tl_ft, 'Transfer Learning (Phase 2 - Fine-tuned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ประเมินผล Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate CNN Model\n",
    "print(\"Evaluating CNN Model...\")\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "print(f\"CNN Test Accuracy: {cnn_accuracy*100:.2f}%\")\n",
    "print(f\"CNN Test Loss: {cnn_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "cnn_predictions = cnn_model.predict(X_test)\n",
    "cnn_pred_classes = np.argmax(cnn_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Transfer Learning Model\n",
    "print(\"Evaluating Transfer Learning Model...\")\n",
    "tl_loss, tl_accuracy = transfer_model.evaluate(X_test_resized, y_test_cat, verbose=0)\n",
    "print(f\"Transfer Learning Test Accuracy: {tl_accuracy*100:.2f}%\")\n",
    "print(f\"Transfer Learning Test Loss: {tl_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "tl_predictions = transfer_model.predict(X_test_resized)\n",
    "tl_pred_classes = np.argmax(tl_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# เปรียบเทียบผลลัพธ์\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Model':<30} {'Accuracy':<15} {'Loss':<10}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'CNN (Custom)':<30} {cnn_accuracy*100:>6.2f}%        {cnn_loss:>6.4f}\")\n",
    "print(f\"{'Transfer Learning (MobileNetV2)':<30} {tl_accuracy*100:>6.2f}%        {tl_loss:>6.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for CNN\n",
    "print(\"\\nCNN Model - Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    cnn_pred_classes, \n",
    "    target_names=[label_map[i] for i in range(24)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report for Transfer Learning\n",
    "print(\"\\nTransfer Learning Model - Classification Report:\")\n",
    "print(classification_report(\n",
    "    y_test, \n",
    "    tl_pred_classes, \n",
    "    target_names=[label_map[i] for i in range(24)]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(16, 14))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[label_map[i] for i in range(24)],\n",
    "                yticklabels=[label_map[i] for i in range(24)],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrix(y_test, cnn_pred_classes, 'CNN Model - Confusion Matrix')\n",
    "plot_confusion_matrix(y_test, tl_pred_classes, 'Transfer Learning Model - Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. บันทึก Model และทดสอบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# บันทึก models\n",
    "cnn_model.save('sign_language_cnn_final.keras')\n",
    "transfer_model.save('sign_language_transfer_final.keras')\n",
    "\n",
    "print(\"Models saved successfully!\")\n",
    "print(\"- sign_language_cnn_final.keras\")\n",
    "print(\"- sign_language_transfer_final.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ฟังก์ชันทดสอบการทำนาย\n",
    "def predict_sign_language(model, image, model_type='cnn'):\n",
    "    \"\"\"\n",
    "    ทำนาย Sign Language จากภาพ\n",
    "    \n",
    "    Args:\n",
    "        model: trained model\n",
    "        image: numpy array ของภาพ (28x28x1)\n",
    "        model_type: 'cnn' หรือ 'transfer'\n",
    "    \"\"\"\n",
    "    # เตรียมภาพ\n",
    "    if model_type == 'transfer':\n",
    "        # Convert to RGB and resize\n",
    "        image_rgb = np.repeat(image, 3, axis=-1)\n",
    "        image_processed = tf.image.resize(image_rgb, [96, 96]).numpy()\n",
    "    else:\n",
    "        image_processed = image\n",
    "    \n",
    "    # Reshape สำหรับการทำนาย\n",
    "    image_input = image_processed.reshape(1, *image_processed.shape)\n",
    "    \n",
    "    # ทำนาย\n",
    "    prediction = model.predict(image_input, verbose=0)\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    return predicted_class, confidence, prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ทดสอบด้วยภาพสุ่มจาก test set\n",
    "def test_random_predictions(n_samples=10):\n",
    "    \"\"\"\n",
    "    ทดสอบการทำนายด้วยภาพสุ่ม\n",
    "    \"\"\"\n",
    "    # สุ่มภาพ\n",
    "    random_indices = np.random.choice(len(X_test), n_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, test_idx in enumerate(random_indices):\n",
    "        # ภาพจริง\n",
    "        image = X_test[test_idx]\n",
    "        true_label = y_test[test_idx]\n",
    "        \n",
    "        # ทำนายด้วย CNN\n",
    "        pred_class_cnn, conf_cnn, _ = predict_sign_language(cnn_model, image, 'cnn')\n",
    "        \n",
    "        # ทำนายด้วย Transfer Learning\n",
    "        pred_class_tl, conf_tl, _ = predict_sign_language(transfer_model, image, 'transfer')\n",
    "        \n",
    "        # แสดงผล\n",
    "        axes[idx].imshow(image.reshape(28, 28), cmap='gray')\n",
    "        \n",
    "        title = f\"True: {label_map[true_label]}\\n\"\n",
    "        title += f\"CNN: {label_map[pred_class_cnn]} ({conf_cnn*100:.1f}%)\\n\"\n",
    "        title += f\"TL: {label_map[pred_class_tl]} ({conf_tl*100:.1f}%)\"\n",
    "        \n",
    "        # เปลี่ยนสีถ้าทำนายผิด\n",
    "        color = 'green' if (pred_class_cnn == true_label and pred_class_tl == true_label) else 'red'\n",
    "        axes[idx].set_title(title, fontsize=10, color=color)\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Random Test Predictions Comparison', y=1.02, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "# ทดสอบ\n",
    "test_random_predictions(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# แสดง Top-3 predictions สำหรับแต่ละภาพ\n",
    "def show_top_predictions(image_idx, model, model_type='cnn', model_name='Model'):\n",
    "    \"\"\"\n",
    "    แสดง Top-3 predictions พร้อมความมั่นใจ\n",
    "    \"\"\"\n",
    "    image = X_test[image_idx]\n",
    "    true_label = y_test[image_idx]\n",
    "    \n",
    "    pred_class, confidence, all_probs = predict_sign_language(model, image, model_type)\n",
    "    \n",
    "    # Top-3\n",
    "    top_3_idx = np.argsort(all_probs)[-3:][::-1]\n",
    "    top_3_probs = all_probs[top_3_idx]\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # แสดงภาพ\n",
    "    ax1.imshow(image.reshape(28, 28), cmap='gray')\n",
    "    ax1.set_title(f'True Label: {label_map[true_label]}', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # แสดง Top-3 predictions\n",
    "    labels = [label_map[i] for i in top_3_idx]\n",
    "    colors = ['green' if top_3_idx[0] == true_label else 'red', 'orange', 'blue']\n",
    "    \n",
    "    ax2.barh(labels, top_3_probs, color=colors)\n",
    "    ax2.set_xlabel('Confidence', fontsize=12)\n",
    "    ax2.set_title(f'{model_name} - Top 3 Predictions', fontsize=14)\n",
    "    ax2.set_xlim([0, 1])\n",
    "    \n",
    "    for i, (label, prob) in enumerate(zip(labels, top_3_probs)):\n",
    "        ax2.text(prob + 0.01, i, f'{prob*100:.1f}%', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ทดสอบกับภาพสุ่ม\n",
    "random_idx = np.random.randint(0, len(X_test))\n",
    "show_top_predictions(random_idx, cnn_model, 'cnn', 'CNN Model')\n",
    "show_top_predictions(random_idx, transfer_model, 'transfer', 'Transfer Learning Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## สรุปผลการทดลอง\n",
    "\n",
    "### Models ที่สร้าง:\n",
    "1. **CNN Model**: Custom CNN architecture\n",
    "2. **Transfer Learning Model**: MobileNetV2 + Fine-tuning\n",
    "\n",
    "### ไฟล์ที่บันทึก:\n",
    "- `best_cnn_model.keras` - Best CNN model during training\n",
    "- `best_transfer_model.keras` - Best Transfer Learning model during training\n",
    "- `sign_language_cnn_final.keras` - Final CNN model\n",
    "- `sign_language_transfer_final.keras` - Final Transfer Learning model\n",
    "\n",
    "### วิธีใช้งาน Model:\n",
    "```python\n",
    "# โหลด model\n",
    "model = keras.models.load_model('sign_language_cnn_final.keras')\n",
    "\n",
    "# เตรียมภาพ (28x28x1, normalized)\n",
    "image = your_image / 255.0\n",
    "image = image.reshape(1, 28, 28, 1)\n",
    "\n",
    "# ทำนาย\n",
    "prediction = model.predict(image)\n",
    "predicted_class = np.argmax(prediction)\n",
    "print(f\"Predicted: {label_map[predicted_class]}\")\n",
    "```\n",
    "\n",
    "### การพัฒนาต่อ:\n",
    "1. ลองใช้ model architectures อื่นๆ (EfficientNet, ResNet)\n",
    "2. เพิ่ม data augmentation techniques\n",
    "3. รวม ensemble models\n",
    "4. สร้าง web application หรือ mobile app\n",
    "5. Real-time sign language detection ด้วย webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สรุปข้อมูล Model\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT SUMMARY: SIGN LANGUAGE RECOGNITION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: Sign Language MNIST\")\n",
    "print(f\"Total Classes: {num_classes} (A-Y, excluding J)\")\n",
    "print(f\"\\nTraining Samples: {len(X_train)}\")\n",
    "print(f\"Validation Samples: {len(X_val)}\")\n",
    "print(f\"Test Samples: {len(X_test)}\")\n",
    "print(f\"\\n{'-'*60}\")\n",
    "print(f\"CNN Model Test Accuracy: {cnn_accuracy*100:.2f}%\")\n",
    "print(f\"Transfer Learning Test Accuracy: {tl_accuracy*100:.2f}%\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"\\nBest Model: {'Transfer Learning' if tl_accuracy > cnn_accuracy else 'CNN'}\")\n",
    "print(f\"Best Accuracy: {max(cnn_accuracy, tl_accuracy)*100:.2f}%\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}